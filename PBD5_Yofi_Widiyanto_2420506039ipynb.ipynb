{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "090da41e",
      "metadata": {
        "id": "090da41e"
      },
      "source": [
        "### 1. Pengenalan MapReduce\n",
        "MapReduce adalah model pemrograman yang digunakan untuk pemrosesan data besar secara paralel di beberapa node dalam kluster Hadoop.\n",
        "- **Map**: Fase pertama di mana data dipecah menjadi unit-unit kecil (key-value pairs).\n",
        "- **Reduce**: Fase kedua di mana hasil dari fase Map dikombinasikan untuk menghasilkan output yang lebih kecil.\n",
        "\n",
        "- **Tugas 1**: Pelajari bagaimana MapReduce bekerja dengan dataset sederhana dan coba implementasikan konsep key-value pair."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7190ed74",
      "metadata": {
        "id": "7190ed74"
      },
      "source": [
        "### 2. Implementasi Sederhana: Word Count\n",
        "Algoritma Word Count adalah salah satu contoh sederhana dari MapReduce. Dalam tugas ini, kita akan menghitung jumlah kata dalam dataset.\n",
        "\n",
        "1. **Map Function**: Fungsi yang memecah teks menjadi kata-kata individual.\n",
        "   ```python\n",
        "   def map_function(text):\n",
        "       for word in text.split():\n",
        "           yield (word, 1)\n",
        "   ```\n",
        "2. **Reduce Function**: Fungsi yang menggabungkan hasil dari fase Map untuk menghitung frekuensi kata.\n",
        "   ```python\n",
        "   from collections import defaultdict\n",
        "\n",
        "   def reduce_function(pairs):\n",
        "       result = defaultdict(int)\n",
        "       for word, count in pairs:\n",
        "           result[word] += count\n",
        "       return result\n",
        "   ```\n",
        "- **Tugas 2**: Implementasikan fungsi `map_function` dan `reduce_function` pada dataset teks sederhana, lalu hitung jumlah kata."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset sederhana\n",
        "dataset = [\"halo hadoop revan hadoop\"]\n",
        "\n",
        "#fungsi map\n",
        "def map_function(text):\n",
        "    for word in text.split():\n",
        "        yield (word, 1)\n",
        "\n",
        "map = []\n",
        "for line in dataset:\n",
        "    map.extend(map_function(line))\n",
        "\n",
        "#menampilkan map\n",
        "print(\"Hasil Map:\")\n",
        "print(map)\n",
        "\n",
        "from collections import defaultdict #mengambil lib\n",
        "\n",
        "#fungsi reduce\n",
        "def reduce_function(pairs):\n",
        "    result = defaultdict(int)\n",
        "    for word, count in pairs:\n",
        "        result[word] += count\n",
        "    return result\n",
        "reduced = reduce_function(map)\n",
        "\n",
        "#menampilkan reduce\n",
        "print(\"\\nHasil Reduce(Word Count):\")\n",
        "for word, count in reduced.items():\n",
        "    print(word, \":\", count)"
      ],
      "metadata": {
        "id": "xhnwgB-yrYl6"
      },
      "id": "xhnwgB-yrYl6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}