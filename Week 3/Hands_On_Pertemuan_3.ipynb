{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f89f7684",
      "metadata": {
        "id": "f89f7684"
      },
      "source": [
        "# Hands-On Pertemuan 3: Data Processing dengan Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e30ce9d1",
      "metadata": {
        "id": "e30ce9d1"
      },
      "source": [
        "## Tujuan:\n",
        "- Memahami dan mempraktikkan data processing menggunakan Apache Spark.\n",
        "- Menggunakan Spark untuk operasi data yang efisien pada dataset besar.\n",
        "- Menerapkan teknik canggih dalam Spark untuk mengatasi kasus penggunaan nyata."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5c2f90",
      "metadata": {
        "id": "cd5c2f90"
      },
      "source": [
        "### 1. Pengenalan Spark DataFrames\n",
        "Spark DataFrame menyediakan struktur data yang optimal dengan operasi yang dioptimalkan untuk pemrosesan data besar, yang sangat mirip dengan DataFrame di Pandas atau di RDBMS.\n",
        "\n",
        "- **Tugas 1**: Buat DataFrame sederhana di Spark dan eksplorasi beberapa fungsi dasar yang tersedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986d01c7",
      "metadata": {
        "id": "986d01c7"
      },
      "outputs": [],
      "source": [
        "# Contoh membuat DataFrame sederhana dan operasi dasar\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('HandsOnPertemuan3').getOrCreate()\n",
        "\n",
        "data = [('James', 'Sales', 3000),\n",
        "        ('Michael', 'Sales', 4600),\n",
        "        ('Robert', 'Sales', 4100),\n",
        "        ('Maria', 'Finance', 3000)]\n",
        "columns = ['EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca66b73",
      "metadata": {
        "id": "fca66b73"
      },
      "source": [
        "### 2. Transformasi Dasar dengan DataFrames\n",
        "Pemrosesan data meliputi transformasi seperti filtering, selections, dan aggregations. Spark menyediakan cara efisien untuk melaksanakan operasi ini.\n",
        "\n",
        "- **Tugas 2**: Gunakan operasi filter, select, groupBy untuk mengekstrak informasi dari data, serta lakukan agregasi data untuk mendapatkan insight tentang dataset menggunakan perintah seperti mean, max, sum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58232678",
      "metadata": {
        "id": "58232678"
      },
      "outputs": [],
      "source": [
        "# Contoh operasi transformasi DataFrame\n",
        "select('EmployeeName', 'Salary').show()\n",
        "filter(df['Salary'] > 3000).show()\n",
        "groupBy('Department').avg('Salary').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89763d36",
      "metadata": {
        "id": "89763d36"
      },
      "source": [
        "### 3. Bekerja dengan Tipe Data Kompleks\n",
        "Spark mendukung tipe data yang kompleks seperti maps, arrays, dan structs yang memungkinkan operasi yang lebih kompleks pada dataset yang kompleks.\n",
        "\n",
        "- **Tugas 3**: Eksplorasi bagaimana mengolah tipe data kompleks dalam Spark DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14701d79",
      "metadata": {
        "id": "14701d79"
      },
      "outputs": [],
      "source": [
        "# Contoh manipulasi tipe data kompleks\n",
        "df.withColumn('SalaryBonus', df['Salary'] * 0.1).show()\n",
        "df.withColumn('TotalCompensation', df['Salary'] + df['SalaryBonus']).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b3b58dd",
      "metadata": {
        "id": "5b3b58dd"
      },
      "source": [
        "### 4. Operasi Data Lanjutan\n",
        "Menggunakan Spark untuk operasi lanjutan seperti window functions, user-defined functions (UDFs), dan mengoptimalkan query.\n",
        "\n",
        "- **Tugas 4**: Implementasikan window function untuk menghitung running totals atau rangkings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "035312eb",
      "metadata": {
        "id": "035312eb"
      },
      "outputs": [],
      "source": [
        "# Contoh menggunakan window functions\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "windowSpec = Window.partitionBy('Department').orderBy('Salary')\n",
        "df.withColumn('Rank', F.rank().over(windowSpec)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8a097ec",
      "metadata": {
        "id": "f8a097ec"
      },
      "source": [
        "### 5. Kesimpulan dan Eksplorasi Lebih Lanjut\n",
        "Review apa yang telah dipelajari tentang pemrosesan data menggunakan Spark dan eksplorasi teknik lebih lanjut untuk mengoptimalkan pemrosesan data Anda.\n",
        "<br>**Tugas 5**:\n",
        "- Unduh dataset besar dari [Kaggle](https://www.kaggle.com/) atau sumber lainnya.\n",
        "- Input data csv yang telah di download, kemudian load dan simpan data ke dalam pyspark.\n",
        "- Setelah data berhasil di load menggunakan pyspark, lakukan manipulasi data untuk memperoleh informasi yang dibutuhkan"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}